{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T12:36:49.452213Z",
     "start_time": "2025-04-03T12:36:49.449754Z"
    }
   },
   "source": "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:36:50.949304Z",
     "start_time": "2025-04-03T12:36:49.485643Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -q -U trl",
   "id": "5b757bffd4b50a4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:36:52.057844Z",
     "start_time": "2025-04-03T12:36:51.014834Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers",
   "id": "f263dd00ed6cc6dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.50.3)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.29.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:36:53.005644Z",
     "start_time": "2025-04-03T12:36:52.067153Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install matplotlib",
   "id": "2ce093fdaf123b90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.10.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.56.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\r\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.2.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (11.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:36:53.915927Z",
     "start_time": "2025-04-03T12:36:53.013417Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install scikit-learn",
   "id": "ceeeb906b04bb365",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:02.398021Z",
     "start_time": "2025-04-03T12:36:53.921767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, TextDataset, DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizer,\n",
    "    Trainer, TrainingArguments, AutoModelForCausalLM\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import Dataset"
   ],
   "id": "e43f96a45f7b3a56",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdw/Documents/Projects/S2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:02.456356Z",
     "start_time": "2025-04-03T12:37:02.404213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"Korean Smile Style Dataset.tsv\", sep=\"\\t\")\n",
    "display(df.head())\n",
    "display(df.isna().mean())\n",
    "display(df.describe())\n",
    "print(df.shape)"
   ],
   "id": "5353832120428c9c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                        formal                    informal  \\\n",
       "0       안녕하세요. 저는 고양이 6마리 키워요.          안녕! 나는 고양이 6마리 키워.   \n",
       "1     고양이를 6마리나요? 키우는거 안 힘드세요?      고양이를 6마리나? 키우는거 안 힘들어?   \n",
       "2  제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.  내가 워낙 고양이를 좋아해서 크게 힘들진 않아.   \n",
       "3       가장 나이가 많은 고양이가 어떻게 돼요?       가장 나이가 많은 고양이가 몇 살이야?   \n",
       "4           여섯 살입니다. 갈색 고양이에요.            여섯 살이야. 갈색 고양이지.   \n",
       "\n",
       "                           android                                   azae  \\\n",
       "0  휴먼. 반갑다. 안드로이드는. 고양이. 6마리. 소유중.  아이고 안녕하십니까~ 나는 그냥 고양이 6마리 키우고 있는 사람이여   \n",
       "1             고양이. 6마리. 양육. 번거로운가.        아니 무슨 고양이를 6마리나? 거 키우는 거 안 힘든가?   \n",
       "2         안드로이드. 고양이. 선호. 힘들지. 않음.        내가 또 워~낙에 고양이를 좋아해서 크게 뭐 힘들진 않고   \n",
       "3           제일. 나이많은. 고양이. 나이. 무엇.                그려 가장 나이가 많은 고양이가 몇살이여?   \n",
       "4                    고양이. 갈색. 여섯살.                        6살인데 갈색 고양이 있어~   \n",
       "\n",
       "                     chat                 choding  \\\n",
       "0     하잉ㅋㅋ 나 떼걸룩 6마리 키운다!      ㅎㅇ 나 주인님 6마리 모심 ㅋㅋ   \n",
       "1       엥? 6마리나? 안힘듬?ㅋㅋㅋㅋ        6마리? 에바아니냐 안 힘듦?   \n",
       "2  내가 고양이 좋아해서 딱히 안힘듬 ㅋㅋㅋ  ㄱㅊ 나 고양이 환장해서 힘든 것도 모름   \n",
       "3     가장 나이 먹은 고양이가 몇살이야?         젤 낡은 고영희가 몇 살임?   \n",
       "4        이제 여섯살이고 갈색고양이임!                 6살, 갈색임   \n",
       "\n",
       "                                  emoticon  \\\n",
       "0        안녕!! >< 나는 고양이😺를 ➏ 마리 키우고있어!! 0_0   \n",
       "1  고양이를 6마리나?!! w(ﾟДﾟ)w 키우는거 안 힘듬?? (⊙_⊙;)   \n",
       "2    뭐 나야 워낙에 고양이 좋아하니까 딱히 안힘드엉! \\(@^0^@)/   \n",
       "3             가장 나이 먹은 고양인 몇 살이양? (´･ω･`)?   \n",
       "4           여설 살!! ㄱ^o^/ 색깔은 갈색! O(*￣▽￣*)ブ   \n",
       "\n",
       "                                       enfp                        gentle  \\\n",
       "0           안녕안녕~! 나 고양이 6마리나 키운다? 완전 대박이징~     안녕하십니까,, 저는 고양이 6마리 키웁니다.   \n",
       "1           고양이를 6마리나? 완전 대박~ 키우는 거 안 힘들어?!     고양이를 6마리나 키우십니까? 안 힘드신지,,   \n",
       "2  내가 또 워~낙에 고양이를 좋아하잖아~ 그렇게 크~게 힘들진 않아 ㅎㅎ~  제가 워낙 고양이를 좋아해서 크게 힘들진 않습니다.   \n",
       "3    대박대박 완전 대박!! 그럼 제~일 나이 많은 고양이는 몇살이야~?!         가장 나이가 있는 고양이가 몇살입니까?   \n",
       "4     6살인 애 있는데, 완전 귀.여.워. 갈색 고양이야 진짜 대박이지?              6살된 갈색 아이가 있습니다.   \n",
       "\n",
       "                                halbae                       halmae  \\\n",
       "0      안녕하신가~... 난 지금 고양이를 6마리 키우고 있다네  하유 시벌것 괭이놈 6마리 키우는데 힘들어 죽겟네   \n",
       "1             고양이를 6마리나? 키우는거 힘들지 않는가?      니기럴 털만 날리는 거 키우기 안 힘들데?   \n",
       "2  내가 워낙에...고양이가 좋아가지고 그렇게 힘들지 않어...^^         옘병 내가 좋아하니까 키워야지 시벌것   \n",
       "3        고양이들 중에서…가장 나이 먹을 애가 몇살인가?...     거 젤 빨리 뒤질 놈이 나이 얼마나 쳐먹었냐   \n",
       "4                  저…갈색 고양이인데…여섯살이지~..   저 노망난 갈색놈이 6살 뒤룩뒤룩 쳐먹은 놈이여   \n",
       "\n",
       "                   joongding                         king  \\\n",
       "0  안녕하냐 ㅡㅡ 나 씹냥이 6마리나 키운다 하;       반갑소. 짐은 고양이를 6마리나 키우오.   \n",
       "1        아니 고양이를 6마리나? 안힘드냐?    고양이를 6마리나? 키우는게 수고스럽진 않소?   \n",
       "2      고양이 좋아한다고ㅡㅡ 1도 안힘듬 ㅡㅡ  과인은 고양이를 어여삐 어겨 그리 수고스럽진 않소   \n",
       "3               가장 늙은애가 몇살인데        최고령 고양이의 나이는 어떻게 되는가?   \n",
       "4                여섯살 갈색냥인데 왜             여섯 살이오. 갈색 고양이오.   \n",
       "\n",
       "                         naruto                            seonbi  \\\n",
       "0   안녕하냐니깐! 난 고양이를 6마리 키우고있다니깐!       안녕하시오! 소인은 고양이를 6마리 키우고 있소!   \n",
       "1     고양이를 6마리나? 키우는거 힘들지 않냐니깐?     고양이를 6마리나 키우고 있는 것이오? 힘들지 않소?   \n",
       "2  내가 고양이를 엄청 좋아해서 별로 힘들지 않다니깐!  소인 고양이를 엄청 좋아하기 때문에 별로 힘들지 않소이다.   \n",
       "3   가장 나이 많이 먹은 고양이가 몇 살 이냐니깐?!          나이를 가장 많이 먹은 고양이가 몇 살이오?   \n",
       "4              갈색 고양이가 여섯살이라니깐!                     여섯 살에 갈색 고양이오   \n",
       "\n",
       "                                 sosim                     translator  \n",
       "0                  안녕… 난 고양이 6마리 키워 ㅠㅠ     반가운. 나는 6마리의 고양이를 소지하고 있다.  \n",
       "1         고양이..6마리나? ㅠ 키우는건 혹시 안힘들어..?  6마리의 고양이? 당신은 그들로부터 지치지 않습니까?  \n",
       "2  내가 고양이 워낙 좋아해서..ㅠㅠ 크게 힘들진 않은 것 같아..        나는 고양이의 큰 애호가. 지치지 않는다.  \n",
       "3        혹시.. 제일 나이 많은 고양이는.. 몇살이야..?ㅠ             가장 늙은 고양이가 몇 년입니까?  \n",
       "4                여섯살이야.. 갈색 ㅠㅠ 고양이야..ㅠ                 여섯. 고양이는 갈색이다.  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formal</th>\n",
       "      <th>informal</th>\n",
       "      <th>android</th>\n",
       "      <th>azae</th>\n",
       "      <th>chat</th>\n",
       "      <th>choding</th>\n",
       "      <th>emoticon</th>\n",
       "      <th>enfp</th>\n",
       "      <th>gentle</th>\n",
       "      <th>halbae</th>\n",
       "      <th>halmae</th>\n",
       "      <th>joongding</th>\n",
       "      <th>king</th>\n",
       "      <th>naruto</th>\n",
       "      <th>seonbi</th>\n",
       "      <th>sosim</th>\n",
       "      <th>translator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>안녕하세요. 저는 고양이 6마리 키워요.</td>\n",
       "      <td>안녕! 나는 고양이 6마리 키워.</td>\n",
       "      <td>휴먼. 반갑다. 안드로이드는. 고양이. 6마리. 소유중.</td>\n",
       "      <td>아이고 안녕하십니까~ 나는 그냥 고양이 6마리 키우고 있는 사람이여</td>\n",
       "      <td>하잉ㅋㅋ 나 떼걸룩 6마리 키운다!</td>\n",
       "      <td>ㅎㅇ 나 주인님 6마리 모심 ㅋㅋ</td>\n",
       "      <td>안녕!! &gt;&lt; 나는 고양이😺를 ➏ 마리 키우고있어!! 0_0</td>\n",
       "      <td>안녕안녕~! 나 고양이 6마리나 키운다? 완전 대박이징~</td>\n",
       "      <td>안녕하십니까,, 저는 고양이 6마리 키웁니다.</td>\n",
       "      <td>안녕하신가~... 난 지금 고양이를 6마리 키우고 있다네</td>\n",
       "      <td>하유 시벌것 괭이놈 6마리 키우는데 힘들어 죽겟네</td>\n",
       "      <td>안녕하냐 ㅡㅡ 나 씹냥이 6마리나 키운다 하;</td>\n",
       "      <td>반갑소. 짐은 고양이를 6마리나 키우오.</td>\n",
       "      <td>안녕하냐니깐! 난 고양이를 6마리 키우고있다니깐!</td>\n",
       "      <td>안녕하시오! 소인은 고양이를 6마리 키우고 있소!</td>\n",
       "      <td>안녕… 난 고양이 6마리 키워 ㅠㅠ</td>\n",
       "      <td>반가운. 나는 6마리의 고양이를 소지하고 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>고양이를 6마리나요? 키우는거 안 힘드세요?</td>\n",
       "      <td>고양이를 6마리나? 키우는거 안 힘들어?</td>\n",
       "      <td>고양이. 6마리. 양육. 번거로운가.</td>\n",
       "      <td>아니 무슨 고양이를 6마리나? 거 키우는 거 안 힘든가?</td>\n",
       "      <td>엥? 6마리나? 안힘듬?ㅋㅋㅋㅋ</td>\n",
       "      <td>6마리? 에바아니냐 안 힘듦?</td>\n",
       "      <td>고양이를 6마리나?!! w(ﾟДﾟ)w 키우는거 안 힘듬?? (⊙_⊙;)</td>\n",
       "      <td>고양이를 6마리나? 완전 대박~ 키우는 거 안 힘들어?!</td>\n",
       "      <td>고양이를 6마리나 키우십니까? 안 힘드신지,,</td>\n",
       "      <td>고양이를 6마리나? 키우는거 힘들지 않는가?</td>\n",
       "      <td>니기럴 털만 날리는 거 키우기 안 힘들데?</td>\n",
       "      <td>아니 고양이를 6마리나? 안힘드냐?</td>\n",
       "      <td>고양이를 6마리나? 키우는게 수고스럽진 않소?</td>\n",
       "      <td>고양이를 6마리나? 키우는거 힘들지 않냐니깐?</td>\n",
       "      <td>고양이를 6마리나 키우고 있는 것이오? 힘들지 않소?</td>\n",
       "      <td>고양이..6마리나? ㅠ 키우는건 혹시 안힘들어..?</td>\n",
       "      <td>6마리의 고양이? 당신은 그들로부터 지치지 않습니까?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>제가 워낙 고양이를 좋아해서 크게 힘들진 않아요.</td>\n",
       "      <td>내가 워낙 고양이를 좋아해서 크게 힘들진 않아.</td>\n",
       "      <td>안드로이드. 고양이. 선호. 힘들지. 않음.</td>\n",
       "      <td>내가 또 워~낙에 고양이를 좋아해서 크게 뭐 힘들진 않고</td>\n",
       "      <td>내가 고양이 좋아해서 딱히 안힘듬 ㅋㅋㅋ</td>\n",
       "      <td>ㄱㅊ 나 고양이 환장해서 힘든 것도 모름</td>\n",
       "      <td>뭐 나야 워낙에 고양이 좋아하니까 딱히 안힘드엉! \\(@^0^@)/</td>\n",
       "      <td>내가 또 워~낙에 고양이를 좋아하잖아~ 그렇게 크~게 힘들진 않아 ㅎㅎ~</td>\n",
       "      <td>제가 워낙 고양이를 좋아해서 크게 힘들진 않습니다.</td>\n",
       "      <td>내가 워낙에...고양이가 좋아가지고 그렇게 힘들지 않어...^^</td>\n",
       "      <td>옘병 내가 좋아하니까 키워야지 시벌것</td>\n",
       "      <td>고양이 좋아한다고ㅡㅡ 1도 안힘듬 ㅡㅡ</td>\n",
       "      <td>과인은 고양이를 어여삐 어겨 그리 수고스럽진 않소</td>\n",
       "      <td>내가 고양이를 엄청 좋아해서 별로 힘들지 않다니깐!</td>\n",
       "      <td>소인 고양이를 엄청 좋아하기 때문에 별로 힘들지 않소이다.</td>\n",
       "      <td>내가 고양이 워낙 좋아해서..ㅠㅠ 크게 힘들진 않은 것 같아..</td>\n",
       "      <td>나는 고양이의 큰 애호가. 지치지 않는다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>가장 나이가 많은 고양이가 어떻게 돼요?</td>\n",
       "      <td>가장 나이가 많은 고양이가 몇 살이야?</td>\n",
       "      <td>제일. 나이많은. 고양이. 나이. 무엇.</td>\n",
       "      <td>그려 가장 나이가 많은 고양이가 몇살이여?</td>\n",
       "      <td>가장 나이 먹은 고양이가 몇살이야?</td>\n",
       "      <td>젤 낡은 고영희가 몇 살임?</td>\n",
       "      <td>가장 나이 먹은 고양인 몇 살이양? (´･ω･`)?</td>\n",
       "      <td>대박대박 완전 대박!! 그럼 제~일 나이 많은 고양이는 몇살이야~?!</td>\n",
       "      <td>가장 나이가 있는 고양이가 몇살입니까?</td>\n",
       "      <td>고양이들 중에서…가장 나이 먹을 애가 몇살인가?...</td>\n",
       "      <td>거 젤 빨리 뒤질 놈이 나이 얼마나 쳐먹었냐</td>\n",
       "      <td>가장 늙은애가 몇살인데</td>\n",
       "      <td>최고령 고양이의 나이는 어떻게 되는가?</td>\n",
       "      <td>가장 나이 많이 먹은 고양이가 몇 살 이냐니깐?!</td>\n",
       "      <td>나이를 가장 많이 먹은 고양이가 몇 살이오?</td>\n",
       "      <td>혹시.. 제일 나이 많은 고양이는.. 몇살이야..?ㅠ</td>\n",
       "      <td>가장 늙은 고양이가 몇 년입니까?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>여섯 살입니다. 갈색 고양이에요.</td>\n",
       "      <td>여섯 살이야. 갈색 고양이지.</td>\n",
       "      <td>고양이. 갈색. 여섯살.</td>\n",
       "      <td>6살인데 갈색 고양이 있어~</td>\n",
       "      <td>이제 여섯살이고 갈색고양이임!</td>\n",
       "      <td>6살, 갈색임</td>\n",
       "      <td>여설 살!! ㄱ^o^/ 색깔은 갈색! O(*￣▽￣*)ブ</td>\n",
       "      <td>6살인 애 있는데, 완전 귀.여.워. 갈색 고양이야 진짜 대박이지?</td>\n",
       "      <td>6살된 갈색 아이가 있습니다.</td>\n",
       "      <td>저…갈색 고양이인데…여섯살이지~..</td>\n",
       "      <td>저 노망난 갈색놈이 6살 뒤룩뒤룩 쳐먹은 놈이여</td>\n",
       "      <td>여섯살 갈색냥인데 왜</td>\n",
       "      <td>여섯 살이오. 갈색 고양이오.</td>\n",
       "      <td>갈색 고양이가 여섯살이라니깐!</td>\n",
       "      <td>여섯 살에 갈색 고양이오</td>\n",
       "      <td>여섯살이야.. 갈색 ㅠㅠ 고양이야..ㅠ</td>\n",
       "      <td>여섯. 고양이는 갈색이다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "formal        0.063428\n",
       "informal      0.063428\n",
       "android       0.520918\n",
       "azae          0.723347\n",
       "chat          0.063428\n",
       "choding       0.063428\n",
       "emoticon      0.514980\n",
       "enfp          0.544130\n",
       "gentle        0.540351\n",
       "halbae        0.515789\n",
       "halmae        0.726586\n",
       "joongding     0.063428\n",
       "king          0.520918\n",
       "naruto        0.514980\n",
       "seonbi        0.514980\n",
       "sosim         0.520918\n",
       "translator    0.594062\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "        formal informal android                                   azae  chat  \\\n",
       "count     3470     3470    1775                                   1025  3470   \n",
       "unique    3430     3417    1748                                   1025  3437   \n",
       "top     안녕하세요.      안녕.    반갑다.  아이고 안녕하십니까~ 나는 그냥 고양이 6마리 키우고 있는 사람이여    하이   \n",
       "freq        23       25      10                                      1    13   \n",
       "\n",
       "       choding      emoticon  enfp  gentle  halbae halmae joongding  king  \\\n",
       "count     3470          1797  1689    1703    1794   1013      3470  1775   \n",
       "unique    3390          1793  1679    1691    1784   1005      3396  1759   \n",
       "top         왜?  안녕! (ﾉ*･ω･)ﾉ   안뇽~  안녕하십니까  안녕하신가…  왜 땜시?        ㅎㅇ  반갑소.   \n",
       "freq        37             3     6       5       8      4        29     7   \n",
       "\n",
       "         naruto  seonbi sosim translator  \n",
       "count      1797    1797  1775       1504  \n",
       "unique     1779    1784  1758       1489  \n",
       "top     안녕하냐니깐!  안녕하시오!  안녕..       반가운.  \n",
       "freq          9       9     9          9  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formal</th>\n",
       "      <th>informal</th>\n",
       "      <th>android</th>\n",
       "      <th>azae</th>\n",
       "      <th>chat</th>\n",
       "      <th>choding</th>\n",
       "      <th>emoticon</th>\n",
       "      <th>enfp</th>\n",
       "      <th>gentle</th>\n",
       "      <th>halbae</th>\n",
       "      <th>halmae</th>\n",
       "      <th>joongding</th>\n",
       "      <th>king</th>\n",
       "      <th>naruto</th>\n",
       "      <th>seonbi</th>\n",
       "      <th>sosim</th>\n",
       "      <th>translator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3470</td>\n",
       "      <td>3470</td>\n",
       "      <td>1775</td>\n",
       "      <td>1025</td>\n",
       "      <td>3470</td>\n",
       "      <td>3470</td>\n",
       "      <td>1797</td>\n",
       "      <td>1689</td>\n",
       "      <td>1703</td>\n",
       "      <td>1794</td>\n",
       "      <td>1013</td>\n",
       "      <td>3470</td>\n",
       "      <td>1775</td>\n",
       "      <td>1797</td>\n",
       "      <td>1797</td>\n",
       "      <td>1775</td>\n",
       "      <td>1504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3430</td>\n",
       "      <td>3417</td>\n",
       "      <td>1748</td>\n",
       "      <td>1025</td>\n",
       "      <td>3437</td>\n",
       "      <td>3390</td>\n",
       "      <td>1793</td>\n",
       "      <td>1679</td>\n",
       "      <td>1691</td>\n",
       "      <td>1784</td>\n",
       "      <td>1005</td>\n",
       "      <td>3396</td>\n",
       "      <td>1759</td>\n",
       "      <td>1779</td>\n",
       "      <td>1784</td>\n",
       "      <td>1758</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>안녕하세요.</td>\n",
       "      <td>안녕.</td>\n",
       "      <td>반갑다.</td>\n",
       "      <td>아이고 안녕하십니까~ 나는 그냥 고양이 6마리 키우고 있는 사람이여</td>\n",
       "      <td>하이</td>\n",
       "      <td>왜?</td>\n",
       "      <td>안녕! (ﾉ*･ω･)ﾉ</td>\n",
       "      <td>안뇽~</td>\n",
       "      <td>안녕하십니까</td>\n",
       "      <td>안녕하신가…</td>\n",
       "      <td>왜 땜시?</td>\n",
       "      <td>ㅎㅇ</td>\n",
       "      <td>반갑소.</td>\n",
       "      <td>안녕하냐니깐!</td>\n",
       "      <td>안녕하시오!</td>\n",
       "      <td>안녕..</td>\n",
       "      <td>반가운.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3705, 17)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:02.594214Z",
     "start_time": "2025-04-03T12:37:02.494345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "row_notna_count = df.notna().sum(axis=1) # df에서 NaN이 아닌 것들을 True로 변경 후, 가로 방향으로 더한 것을 dataframe으로 나타낸 것\n",
    "row_notna_count.plot.hist(bins=row_notna_count.max()) # 히스토그램의 가로축 범위를 지정하는 bins를 사용하여 가로축 범위를 row_notna_count의 최댓값인 17로 설정되게 함.\n",
    "plt.show() # 표 출력\n",
    "\n",
    "df = df[row_notna_count >= 2] # NaN이 아닌 값이 2개 이상의 열에 들어있는 것만 df에 입력\n",
    "print(len(df))"
   ],
   "id": "8aacdbe91fdc9d89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3470\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:03.355066Z",
     "start_time": "2025-04-03T12:37:02.600284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "a7174082a3821f14",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:03.759958Z",
     "start_time": "2025-04-03T12:37:03.366704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_dropped = df.drop([\"emoticon\"], axis=1) # emoticon 말투를 제외한 데이터셋\n",
    "lengths = []\n",
    "\n",
    "for column in df_dropped.columns:\n",
    "  out = tokenizer(df[column][df[column].notna()].tolist()) # df[column].notna()를 해서 True로 나오는 NaN이 아닌 값들을 모아 list 형태로 변환하여 tokenizer에 전송\n",
    "  out = [len(x) for x in out['input_ids']] # 토큰화된 문장들의 길이를 리스트로 저장\n",
    "  lengths.extend(out) # 리스트로 추가하는 것이 아닌 각각의 숫자로 lengths 리스트에 추가 (append, extend 차이 검색)\n",
    "\n",
    "lengths = pd.Series(lengths) # lengths를 데이터프레임 형식으로 변환"
   ],
   "id": "bdca675e55a56edc",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:03.833082Z",
     "start_time": "2025-04-03T12:37:03.772009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "display(lengths.describe()) # 토큰화된 문장들의 특징들을 표시\n",
    "lengths.plot.hist(bins=80) # 히스토그램에서 80개의 막대로 lengths의 데이터 분포를 그래프로 출력"
   ],
   "id": "22af75b5c30b5da9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    34996.000000\n",
       "mean        18.297377\n",
       "std          8.625557\n",
       "min          2.000000\n",
       "25%         12.000000\n",
       "50%         17.000000\n",
       "75%         23.000000\n",
       "max         97.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:03.847684Z",
     "start_time": "2025-04-03T12:37:03.845348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextStyleTransferDataset(Dataset): \n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizer, tokenizer_params: Dict = {}):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_params = tokenizer_params\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)  # 데이터프레임 길이 반환\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index, :].dropna().sample(2)  # 샘플 2개 선택\n",
    "        text1 = row.iloc[0]  \n",
    "        text2 = row.iloc[1]  \n",
    "        target_style = row.index[1]  # 스타일 정보 저장\n",
    "\n",
    "        line = f\"{text1}[{target_style}]{text2}[SEP]\"\n",
    "        out = self.tokenizer(line, **self.tokenizer_params)  # 토큰화 수행\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in out.items()}  # PyTorch 텐서로 변환"
   ],
   "id": "95d121835a674d2a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:03.863702Z",
     "start_time": "2025-04-03T12:37:03.859220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_params = dict(\n",
    "    return_tensors='pt', # 파라미터를 pytorch tensor로 반환\n",
    "    truncation=True, # token화된 문장을 max_length 길이로 자르기 (긴 경우)\n",
    "    padding=\"max_length\", # max_length 길이로 패딩 (짧은 경우)\n",
    "    add_special_tokens=True, # 함수 내부에서 자동적으로 [CLS], [SEP] 토큰을 추가해줌\n",
    "    max_length=50 # max_length 지정\n",
    ")\n",
    "\n",
    "dataset = TextStyleTransferDataset(df_dropped, tokenizer, tokenizer_params) # 이모티콘을 제외한 데이터프레임을 전달하여 토큰 임베딩\n",
    "out = dataset[0]\n",
    "print(out['input_ids'][0])\n",
    "print(tokenizer.decode(out['input_ids'][0]))\n",
    "\n",
    "out = dataset[1]\n",
    "print(out['input_ids'][0])\n",
    "print(tokenizer.decode(out['input_ids'][0]))"
   ],
   "id": "ab5b23a8daef54b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128000, 101193, 124409,     13, 102678,  16969, 101254, 101927,  13094,\n",
      "           220,     21, 100711,  29102, 108652, 103430,  36811,   8032,    331,\n",
      "          3785,     60, 116507, 100900,  74618,  56773,  32428, 102424,    220,\n",
      "            21, 100711,  29102,  55170, 102612, 125736,     58,  82476,     60,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009])\n",
      "<|begin_of_text|>안녕하세요. 저는 고양이 6마리 키워요.[choding]ㅎㅇ 나 주인님 6마리 모심 ㅋㅋ[SEP]<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "tensor([128000,  35495, 101927, 119165,    220,     21, 100711,  29102,  61415,\n",
      "            30, 108652,  41381,  16969,  93292, 110671,  65950,  22035,  51796,\n",
      "        106888,  84136,  84291,    238,  61864,  12130, 117171,     60,  84136,\n",
      "         21121, 121408, 111109,    116,  73653, 105605, 104374, 101429, 108652,\n",
      "         41381,  21121,  96270, 110671,  65950, 100933,  61864,  82476,     60,\n",
      "        128009, 128009, 128009, 128009, 128009])\n",
      "<|begin_of_text|>고양이를 6마리나? 키우는거 힘들지 않냐니깐?[halmae]니기럴 털만 날리는 거 키우기 안 힘들데?[SEP]<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/0bw1kcd572v6_tw8vvdwhb7r0000gn/T/ipykernel_18037/3291157668.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(value) for key, value in out.items()}  # PyTorch 텐서로 변환\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:03.913945Z",
     "start_time": "2025-04-03T12:37:03.876762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_dropped, test_size=0.1, random_state=42) # 학습데이터, 테스트데이터 분리\n",
    "print(len(df_train), len(df_test))"
   ],
   "id": "bdb8a1e9eb1ffb75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3123 347\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:03.930584Z",
     "start_time": "2025-04-03T12:37:03.928715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_params = dict(\n",
    "    return_tensors='pt',\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=100\n",
    ")\n",
    "train_dataset = TextStyleTransferDataset(\n",
    "    df_train, # 학습데이터를 토큰화\n",
    "    tokenizer,\n",
    "    tokenizer_params\n",
    ")\n",
    "test_dataset = TextStyleTransferDataset(\n",
    "    df_test, # 테스트데이터 토큰화\n",
    "    tokenizer,\n",
    "    tokenizer_params\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, # 토큰 자동 패딩을 허용하지 않음\n",
    ")"
   ],
   "id": "46ffc633d5d8c771",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:11.797535Z",
     "start_time": "2025-04-03T12:37:03.944943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ],
   "id": "124893789c1e860",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:12.440903Z",
     "start_time": "2025-04-03T12:37:11.843764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"/Users/kdw/Documents/Projects/S2/data\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path, #The output directory / 모델 학습 이후 weight 저장 경로 지정\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory / weight 파일이 존재하면 덮어쓰기\n",
    "    num_train_epochs=40, # number of training epochs / 전체 데이터셋이 신경망을 40번 통과하게 함\n",
    "    per_device_train_batch_size=16, # batch size for training / 학습시킬 때 사용할 배치 사이즈 (한번에 학습시킬 데이터의 크기)\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation / 결과 낼 때 사용할 배치 사이즈\n",
    "    eval_steps=500, # Number of update steps between two evaluations. / 500번의 배치마다 평가를 함\n",
    "    save_steps=500, # after # steps model is saved / 500번의 배치마다 체크포인트를 저장함\n",
    "    warmup_steps=300,# number of warmup steps for learning rate scheduler / 300번의 배치까지 점진적으로 학습률을 올리다가 그 후로는 steps의 규칙에 따라 학습하게 함\n",
    "    prediction_loss_only=True, # prediction을 실행할 때마다 loss값만 반환하게 함\n",
    "    evaluation_strategy=\"steps\" # 평가를 어떠한 형식으로 할 것인지 설정. (no, steps, epoch로 설정할 수 있으며, no로 정해둘 경우 eval_steps와 save_steps가 의미 없어짐.)\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model, # 지정한 모델로 학습할 것이라고 설정\n",
    "    # args=training_args, # 위에서 설정한 arguments(전달 인자)를 Trainer로 입력\n",
    "    args=SFTConfig(output_dir=\"./ckpt\"),\n",
    "    data_collator=data_collator, # 패딩, 마스킹 등 전처리 작업을 설정한 값을 전달\n",
    "    train_dataset=train_dataset, # 학습 데이터셋 전달\n",
    "    formatting_func=None\n",
    "    # eval_dataset=test_dataset, # 평가 데이터셋 전달\n",
    ")"
   ],
   "id": "a09384095aa37c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdw/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/lc/0bw1kcd572v6_tw8vvdwhb7r0000gn/T/ipykernel_18037/3291157668.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(value) for key, value in out.items()}  # PyTorch 텐서로 변환\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextStyleTransferDataset' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m      1\u001B[39m model_path = \u001B[33m\"\u001B[39m\u001B[33m/Users/kdw/Documents/Projects/S2/data\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      3\u001B[39m training_args = TrainingArguments(\n\u001B[32m      4\u001B[39m     output_dir=model_path, \u001B[38;5;66;03m#The output directory / 모델 학습 이후 weight 저장 경로 지정\u001B[39;00m\n\u001B[32m      5\u001B[39m     overwrite_output_dir=\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;66;03m#overwrite the content of the output directory / weight 파일이 존재하면 덮어쓰기\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m     evaluation_strategy=\u001B[33m\"\u001B[39m\u001B[33msteps\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;66;03m# 평가를 어떠한 형식으로 할 것인지 설정. (no, steps, epoch로 설정할 수 있으며, no로 정해둘 경우 eval_steps와 save_steps가 의미 없어짐.)\u001B[39;00m\n\u001B[32m     14\u001B[39m     )\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m trainer = \u001B[43mSFTTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# 지정한 모델로 학습할 것이라고 설정\u001B[39;49;00m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# args=training_args, # 위에서 설정한 arguments(전달 인자)를 Trainer로 입력\u001B[39;49;00m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mSFTConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./ckpt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# 패딩, 마스킹 등 전처리 작업을 설정한 값을 전달\u001B[39;49;00m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# 학습 데이터셋 전달\u001B[39;49;00m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mformatting_func\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# eval_dataset=test_dataset, # 평가 데이터셋 전달\u001B[39;49;00m\n\u001B[32m     24\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:194\u001B[39m, in \u001B[36mSFTTrainer.__init__\u001B[39m\u001B[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001B[39m\n\u001B[32m    192\u001B[39m preprocess_dataset = args.dataset_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args.dataset_kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mskip_prepare_dataset\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    193\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m preprocess_dataset:\n\u001B[32m--> \u001B[39m\u001B[32m194\u001B[39m     train_dataset = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prepare_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessing_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpacking\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatting_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m    196\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    197\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m eval_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    198\u001B[39m         packing = args.packing \u001B[38;5;28;01mif\u001B[39;00m args.eval_packing \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m args.eval_packing\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:483\u001B[39m, in \u001B[36mSFTTrainer._prepare_dataset\u001B[39m\u001B[34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001B[39m\n\u001B[32m    481\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset, Dataset):  \u001B[38;5;66;03m# `IterableDataset.map` does not support `desc`\u001B[39;00m\n\u001B[32m    482\u001B[39m         map_kwargs[\u001B[33m\"\u001B[39m\u001B[33mdesc\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTruncating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m dataset\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m483\u001B[39m     dataset = \u001B[43mtruncate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[38;5;66;03m# For Liger kernel, ensure only input_ids is present\u001B[39;00m\n\u001B[32m    485\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m args.use_liger_kernel:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/trl/data_utils.py:592\u001B[39m, in \u001B[36mtruncate_dataset\u001B[39m\u001B[34m(dataset, max_length, map_kwargs)\u001B[39m\n\u001B[32m    589\u001B[39m             truncated_examples[key] = column\n\u001B[32m    590\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m truncated_examples\n\u001B[32m--> \u001B[39m\u001B[32m592\u001B[39m     dataset = \u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m(\n\u001B[32m    593\u001B[39m         truncate,\n\u001B[32m    594\u001B[39m         batched=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    595\u001B[39m         **map_kwargs,\n\u001B[32m    596\u001B[39m     )\n\u001B[32m    597\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "\u001B[31mAttributeError\u001B[39m: 'TextStyleTransferDataset' object has no attribute 'map'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T12:37:12.444899Z",
     "start_time": "2025-03-29T13:21:24.963023Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train() # 학습 시작",
   "id": "956599591c706826",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103\n",
      "1004\n",
      "1432\n",
      "2737\n",
      "410\n",
      "2868\n",
      "2209\n",
      "2863\n",
      "2388\n",
      "1154\n",
      "2189\n",
      "1489\n",
      "2354\n",
      "557\n",
      "3044\n",
      "955\n",
      "1467\n",
      "2482\n",
      "2910\n",
      "1621\n",
      "2196\n",
      "1085\n",
      "2745\n",
      "134\n",
      "722\n",
      "696\n",
      "492\n",
      "1116\n",
      "1915\n",
      "1536\n",
      "441\n",
      "1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/0bw1kcd572v6_tw8vvdwhb7r0000gn/T/ipykernel_37657/1457535890.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text1 = row[0] # 첫 번째 샘플\n",
      "/var/folders/lc/0bw1kcd572v6_tw8vvdwhb7r0000gn/T/ipykernel_37657/1457535890.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text2 = row[1] # 두 번째 샘플\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(MPSFloatType{[16, 16, 1, 1, 100]}, size=[16, 1, 1, 100]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (5)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[52]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# 학습 시작\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2243\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2244\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2245\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2246\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2250\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/trainer.py:2556\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2549\u001B[39m context = (\n\u001B[32m   2550\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2551\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2552\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2553\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2554\u001B[39m )\n\u001B[32m   2555\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2556\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2558\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2559\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2560\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2561\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2562\u001B[39m ):\n\u001B[32m   2563\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2564\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/trainer.py:3718\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3715\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb.reduce_mean().detach().to(\u001B[38;5;28mself\u001B[39m.args.device)\n\u001B[32m   3717\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.compute_loss_context_manager():\n\u001B[32m-> \u001B[39m\u001B[32m3718\u001B[39m     loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3720\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[32m   3721\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   3722\u001B[39m     \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3723\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.global_step % \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps == \u001B[32m0\u001B[39m\n\u001B[32m   3724\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/trainer.py:3783\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3781\u001B[39m         loss_kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_items_in_batch\u001B[39m\u001B[33m\"\u001B[39m] = num_items_in_batch\n\u001B[32m   3782\u001B[39m     inputs = {**inputs, **loss_kwargs}\n\u001B[32m-> \u001B[39m\u001B[32m3783\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3784\u001B[39m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[32m   3785\u001B[39m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[32m   3786\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.past_index >= \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:853\u001B[39m, in \u001B[36mLlamaForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    850\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m    852\u001B[39m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m853\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    854\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    855\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    856\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    857\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    858\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    859\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    860\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    861\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    862\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    863\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    864\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    865\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    867\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    868\u001B[39m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:571\u001B[39m, in \u001B[36mLlamaModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[39m\n\u001B[32m    568\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m position_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    569\u001B[39m     position_ids = cache_position.unsqueeze(\u001B[32m0\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m571\u001B[39m causal_mask = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_update_causal_mask\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    575\u001B[39m hidden_states = inputs_embeds\n\u001B[32m    577\u001B[39m \u001B[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:678\u001B[39m, in \u001B[36mLlamaModel._update_causal_mask\u001B[39m\u001B[34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001B[39m\n\u001B[32m    671\u001B[39m     target_length = (\n\u001B[32m    672\u001B[39m         attention_mask.shape[-\u001B[32m1\u001B[39m]\n\u001B[32m    673\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(attention_mask, torch.Tensor)\n\u001B[32m    674\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m past_seen_tokens + sequence_length + \u001B[32m1\u001B[39m\n\u001B[32m    675\u001B[39m     )\n\u001B[32m    677\u001B[39m \u001B[38;5;66;03m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m678\u001B[39m causal_mask = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prepare_4d_causal_attention_mask_with_cache_position\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    679\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    680\u001B[39m \u001B[43m    \u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    681\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtarget_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    682\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    683\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    684\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    685\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    686\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    688\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    689\u001B[39m     \u001B[38;5;28mself\u001B[39m.config._attn_implementation == \u001B[33m\"\u001B[39m\u001B[33msdpa\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    690\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    695\u001B[39m     \u001B[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001B[39;00m\n\u001B[32m    696\u001B[39m     \u001B[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001B[39;00m\n\u001B[32m    697\u001B[39m     min_dtype = torch.finfo(dtype).min\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Projects/S2/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:754\u001B[39m, in \u001B[36mLlamaModel._prepare_4d_causal_attention_mask_with_cache_position\u001B[39m\u001B[34m(attention_mask, sequence_length, target_length, dtype, device, cache_position, batch_size, **kwargs)\u001B[39m\n\u001B[32m    750\u001B[39m         padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, :].to(\n\u001B[32m    751\u001B[39m             causal_mask.device\n\u001B[32m    752\u001B[39m         )\n\u001B[32m    753\u001B[39m         padding_mask = padding_mask == \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m754\u001B[39m         \u001B[43mcausal_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43mmask_length\u001B[49m\u001B[43m]\u001B[49m = causal_mask[:, :, :, :mask_length].masked_fill(\n\u001B[32m    755\u001B[39m             padding_mask, min_dtype\n\u001B[32m    756\u001B[39m         )\n\u001B[32m    758\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m causal_mask\n",
      "\u001B[31mRuntimeError\u001B[39m: expand(MPSFloatType{[16, 16, 1, 1, 100]}, size=[16, 1, 1, 100]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (5)"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b08619c2f04fa9de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
